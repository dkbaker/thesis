{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import random\n",
    "from gensim import corpora, models\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants \n",
    "\n",
    "NUMDOCS = 60 # number of subreddits OR number of BBC documents per category\n",
    "NUMTOPICS = 30 # number of topics to extract \n",
    "COMMENTLIMIT = 1000 # for reddit only; number of comments to represent one subreddit\n",
    "\n",
    "REDDIT = \"REDDIT\"\n",
    "BBC = \"BBC\"\n",
    "\n",
    "REDDIT_PATH = \"/texts/reddit-comments.sqlite\"\n",
    "TOP_SUBREDDITS = '/misc/top_100_subreddits'\n",
    "BBC_PATH = \"/texts/bbc-fulltext\"\n",
    "\n",
    "# we needed to add additional stop words to fully denoise the data\n",
    "# this includes stripping URLs includes reddit-specific terms\n",
    "\n",
    "ADDNL_STOP_WORDS = [u's',u'www',u'year', u'com', u'http', u'https', u'use', u'make', u'know', u'say', \\\n",
    "                  u'even', u'go', u'think', u'', u't',u're', u'said', u'will', u'like', \\\n",
    "                  u'just', u'also', u'can', u'get', u'don', u'delete', \\\n",
    "                  u'really', u'good', u'know', u'think', u'one', u'even', u'need', u'way',\\\n",
    "                  u'want', u'people', u'thing', u'look', u'work', u'time', \\\n",
    "                  u'see', u'reddit', u'using', u'wants', u'comment', \\\n",
    "                 u'please', u'looks', u'looking', u'message',\\\n",
    "                 u'gt', u'messag', u'pleas', u'way', u'someth', u've', u'remov', u'well', u'take',\\\n",
    "                 u'now', u'post', u'still', u'try', u'tri', u'right', u'd', u'much', u'person',\\\n",
    "                 u'submit', u'submission', u'subreddit', u'doesn', u'isn', u'sure', u'didn', u'll',\\\n",
    "                 u'got', u'u', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'0', u'back',\\\n",
    "                 u'come', u'v', u'a', u'b', u'c', u'd', u'e', u'f', u'g', u'h', u'i', u'j', u'k',\\\n",
    "                 u'l', u'm', u'n', u'o', u'p', u'q', u'r', u's', u't', u'u', u'v', u'w', u'x', u'y',\\\n",
    "                 u'z']\n",
    "\n",
    "def toVector(l):\n",
    "    ''' inserts 0s for unrepresented topics in a document\n",
    "        input: return value from ldamodel.get_document_topics(corpus[i])\n",
    "        output: document topics with 0s for unrepresented topics'''\n",
    "    ret = []\n",
    "    t = 0\n",
    "    for i in range(NUMTOPICS):\n",
    "        if t == len(l):\n",
    "            ret += [0]\n",
    "        elif l[t][0] == i + 1:\n",
    "            ret += [l[t][1]]\n",
    "            t = t + 1\n",
    "        else:\n",
    "            ret += [0]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define which dataset you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = BBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data == BBC:\n",
    "    \n",
    "    # read docs from bbc\n",
    "    docs = []\n",
    "\n",
    "    # ignore system files\n",
    "    dirs = [f for f in os.listdir(BBC_PATH) if not f.startswith('.')]\n",
    "    \n",
    "    # keep track of document labels\n",
    "    doc_names = []\n",
    "    for d in dirs:\n",
    "        doc_names += [str(d)]\n",
    "        \n",
    "    # how many documents should we get?\n",
    "    if NUMDOCS == \"ALL\":\n",
    "        maxDocs = float(\"inf\")\n",
    "    else:\n",
    "        maxDocs = NUMDOCS\n",
    "    \n",
    "    # extract all of the documents, recording how many were in \n",
    "    # each category for labelling later\n",
    "    doc_id_counts = [0 for x in range(len(doc_names))]\n",
    "    currentDocID = 0\n",
    "    for d in dirs:\n",
    "        i = 0\n",
    "        for f in os.listdir(os.path.join(path,d)):\n",
    "            doc_id_counts[currentDocID] += 1\n",
    "            docs += [open(os.path.join(path,d,f), \"r\").read()]\n",
    "            i = i + 1\n",
    "            if i >= maxDocs:\n",
    "                break\n",
    "        currentDocID += 1\n",
    "    str_docs = docs\n",
    "    \n",
    "elif data == REDDIT:\n",
    "    \n",
    "    # read in the names of the most popular subreddits\n",
    "    with open(REDDIT_PATH) as f:\n",
    "        top_subreddits = f.read().splitlines()\n",
    "\n",
    "    # pull the data from reddit\n",
    "    # each subreddit will be read as a 'document' for LDA analysis\n",
    "    sql_conn = sqlite3.connect(REDDIT_PATH)\n",
    "    docs = []\n",
    "    for sub in top_subreddits[0:NUMDOCS]:\n",
    "        docs += [pd.read_sql(\n",
    "                \"SELECT body FROM May2015 WHERE subreddit = '\" + str(sub) + \n",
    "                \"' LIMIT \" + str(COMMENTLIMIT), \n",
    "                sql_conn)]\n",
    "    \n",
    "    # turn the dataframes into arrays, then into long strings\n",
    "    # note that each subreddit is represented by a concatenation of comments\n",
    "    str_docs = []\n",
    "    for i in range(len(docs)):\n",
    "        tostring = \"\"\n",
    "        for e in docs[i].as_matrix():\n",
    "            tostring += e\n",
    "            tostring += \" \"\n",
    "        str_docs += [tostring]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "# loop through document list\n",
    "\n",
    "for doc in str_docs:\n",
    "\n",
    "    if data == REDDIT:\n",
    "    # clean and tokenize document string\n",
    "        raw = doc[0].lower()\n",
    "    else:\n",
    "        raw = doc.lower()\n",
    "        raw = raw.decode(\"utf8\", errors=\"ignore\")\n",
    "    #print raw\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    en_stop = get_stop_words('en') + ADDNL_STOP_WORDS\n",
    "\n",
    "    # remove stop words\n",
    "    stopped_tokens = []\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    p_stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # ensure we didn't miss any stop words, after stemming\n",
    "    restopped_tokens = [i for i in stemmed_tokens if not i in en_stop]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(restopped_tokens)\n",
    "    \n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate LDA model across texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                           num_topics=NUMTOPICS, \n",
    "                                           id2word = dictionary, passes=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a list of \"pretty\" topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you can get topic proportions with ldamodel.print_topics()\n",
    "for i in range(0, ldamodel.num_topics):\n",
    "    tops = []\n",
    "    filtered = re.findall( '\\\"[a-z]+\\\"', ldamodel.print_topic(i, 20))\n",
    "    for elem in filtered:\n",
    "        print elem + \", \",\n",
    "    print \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate similarity between all document topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# include a small value to add to all distance measurements\n",
    "# such that we never get 0 distance\n",
    "EPS = 0.001\n",
    "\n",
    "# note: this can take a long time on large corpora\n",
    "similarity = [[0 for x in range(numDocs)] for y in range(numDocs)]\n",
    "\n",
    "for i in range(numDocs):\n",
    "    itops = toVector(ldamodel.get_document_topics(corpus[i]))\n",
    "    for j in range(numDocs)[i:]:\n",
    "        jtops = toVector(ldamodel.get_document_topics(corpus[j]))\n",
    "        sim = 1.0 - gensim.matutils.hellinger(itops, jtops)\n",
    "        sim = min(sim + EPS, 1.0)\n",
    "        \n",
    "        similarity[i][j] = sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color and label everything based on its source for BBC data or based on its topic for Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data == BBC:\n",
    "    # generate colors for each topic\n",
    "    NUMCOLORS = len(doc_names)\n",
    "    raw_cols = []\n",
    "\n",
    "    # these are default placeholder colors\n",
    "    # we can change these colors later in Gensim\n",
    "    raw_cols = [\"#ff0d05\", \"#ff9a02\", \"#00ff00\", \"#00e1ff\", \"#ff00ff\"]\n",
    "\n",
    "    colors = {}\n",
    "    labels = {}\n",
    "\n",
    "    i = 0\n",
    "    doc = 0\n",
    "    counter = [x for x in doc_id_counts]\n",
    "\n",
    "    while i < len(counter):\n",
    "        labels[doc] = doc_names[i]\n",
    "        if counter[i] > 0:\n",
    "            counter[i] = counter[i] - 1\n",
    "        else:\n",
    "            i = i + 1      \n",
    "        doc = doc + 1\n",
    "\n",
    "    for i in range(len(doc_names)):\n",
    "        colors[doc_names[i]] = raw_cols[i]\n",
    "\n",
    "if data == REDDIT:\n",
    "    \n",
    "    # generate colors for each topic\n",
    "    NUMCOLORS = NUMTOPICS\n",
    "    raw_cols = []\n",
    "    currentcol = 5.0\n",
    "    for i in range(NUMCOLORS):\n",
    "        raw_cols += [\"#%06x\" % random.randint(0xa982ff, 0xFFFFFF)]\n",
    "\n",
    "    # get node data\n",
    "    colors = {}\n",
    "    labels = {}\n",
    "    doctopics = {}\n",
    "    for i in range(len(str_docs)):\n",
    "        labels[i] = top_subreddits[i]\n",
    "        doctopic = ldamodel.get_document_topics(corpus[i])[0][0] % len(raw_cols)\n",
    "        colors[i] = raw_cols[doctopic]\n",
    "        doctopics[i] = doctopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create networkx graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "          \n",
    "if data == BBC:\n",
    "\n",
    "    for i in range(len(str_docs)):\n",
    "        G.add_node(i, {'label': labels[i], 'color' : colors[labels[i]]})\n",
    "\n",
    "    inc = 0\n",
    "    for i in range(len(str_docs)):\n",
    "        for j in range(len(str_docs))[i+1:]:\n",
    "            inc = inc + 1\n",
    "            if similarity > 0:\n",
    "                G.add_edge(i, j, weight = similarity[i][j])\n",
    "                \n",
    "elif data == REDDIT:\n",
    "    for i in range(len(str_docs)):\n",
    "        G.add_node(i, {'label': labels[i], 'color' : colors[i], 'topic' : doctopics[i]})\n",
    "    for i in range(len(str_docs)):\n",
    "            for j in range(len(str_docs))[i+1:]:\n",
    "                G.add_edge(i, j, weight = similarity[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save to gexf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"%s_%d-docs_%d-topics\"% (data, NUMDOCS, NUMTOPICS)\n",
    "nx.write_gexf(G, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, the remainder of the graph creation process happens in Gensim!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
